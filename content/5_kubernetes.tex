\chapter{Kubernetes}\label{ch:kubernetes}

\section{Overview}

Kubernetes is a container platform for high availability clusters.
There exist plugins for integration tests for Kubernetes with the name kubetest\footnote{\url{https://kubetest.readthedocs.io/en/latest/}}. They contain conformance tests, as well as e2e tests (end-to-end).
That can be all built and executed on the system. Therefore, a Dockerfile for setting up Kubernetes and building tests with Go is necessary. The challenge is, that 2 big Github repositories have to be cloned and integrated into the docker image. These are using a lot of space. One solution is using a multi staging Dockerfile. 
So 2 different Dockerfiles are used in one Dockerfile and one of them is used for building. The other one is used for the installation and testing with built tests. At the end the size of the docker image has got only the size of the test image regardless of the repository size in the mother Dockerfile.

\section{Docker Multi-Arch Image}
As mentioned in \ref{Docker-Intro} "Introduction of Docker", Docker is used as a base container engine for Kubernetes tests by the community. \\  
Docker Inc. is maintaining a Docker image with the name "Docker-in-Docker" for multiple architectures, incl. s390x. 
This Docker image was introduced by the Docker contributor Jerome Petazzoni (Tianon) with the name dind\footnote{\url{https://github.com/jpetazzo/dind}}. Docker can be installed on Ubuntu, openSUSE, Fedora, ArchLinux and Alpine on this way.
The base Docker image on Docker Hub\footnote{\url{https://hub.docker.com/_/docker}} is based on Ubuntu.
The base image for a specified architecture can be integrated with 2 methods. Firstly, the s390x base image can be pulled to the local container registry with \textbf{docker pull s390x/docker}. The specification \textbf{docker} behind pull alone would download the Docker image for the host architecture or would choose the platform for a multi-arch build with the platform option of BuildX in the command \textbf{docker build}. The prefix \textbf{s390x} before docker specifies a specific available architecture. 
The second option is the integration of \textbf{s390x/docker} into the FROM command inside of the Dockerfile. Both methods are downloading the latest Docker version with Ubuntu from \path{docker.io/s390x/docker:latest} and register this Docker image in the local registry.
Therefore, this image is used as a base image in the self-developed Dockerfile for Kubernetes.



\section{Multi-Staging Dockerfile}

A multi-staging Dockerfile is using different systems in one Dockerfile for different stages. These systems are receiving special names as indicators with "AS" behind the "FROM" with the base image name. 
Default this feature is used for building applications in one stage and executing the copied application in another stage. The same counts for cloning Github repositories and building binary files based on it. On this way, a lot of space is saved.
Concluding, the docker image has got only the size of the executing system with the application file (without all the code). 
That is an "experimental feature"  at the moment. Therefore the \textbf{experimental flag} is necessary to export or set before using it (see \ref{Multi-Architecture-Images} "Multi-Architecture Images"). \\
Default one image is receiving the name build and the other one a name of what will be done with that. In our case, the second image has got the name work. 
The second image is copying with "COPY --from=build" all required built data from the first image that it can be used for running the application. 
On this way, it is possible to reduce the size of a docker image. \\
Multi-staging Dockerfiles are an approved method for executing binaries based on Github repositories.


\section{Installation}

Kubernetes requires a lot of packages for running and for tests. That will be all installed with the RUN command.
\url{apt.kubernetes.io} has got later versions of Kubernetes than the Ubuntu repository. Therefore this repository has to be added to Ubuntu. kub-build is the name of the mother Dockerfile to be able to copy needed files and directories from there. \\
In the intallation part of the Dockerfile the Kubernetes repository \url{https://apt.kubernetes.io} for Debian packages has to be registered together with with the gpg key used by  \url{https://packages.cloud.google.com/apt/doc/apt-key.gpg}.
A Dockerfile is installing only necessary packages. Therefore, apt-transport-https, apt-utils, curl, git, ca-certificates, gnupg-agent and software-properties-common have to be installed first for the following installation.
The system requires the lates update with \lstinline!apt-get update! after the registration of the additional repository besides of the default Ubuntu repositories imported with the base Ubuntu image "s390x/ubuntu:18.04" in the FROM command. After that the packages docker.io, kubelet and kubeadm can be installed from kubernetes.io. \textbf{Docker.io} contains the container engine docker with all docker commands. CRI/O or containerd would be allowed, too. 
The Docker daemon will be used because that is the main used container engine of the Kubernetes project and all tests are running with it. \\ \textbf{Kubelet} is the primary node agent running on each node. He is responsible that different containers can run together in a pod. Pods are deployable units defined in JSON or a yaml file. 
They include one or a group of containers with shared storage and network resources. Hosting of distributed systems  with different services in different containers can work together. Consequential one pod is soething as one "logical host". \\
\textbf{Kubeadm} is the administration tool to setup clusters. It is necessary to upgrade Kubernetes to other versions. Clusters can be initialized. The network can be configured and the command \textbf{kubectl} (Kubernetes Control Plane) for adding additional nodes to a cluster can be initialized. \\
\lstinline!apt-mark hold! is keeping these special versions of kubelet, kubeadm and kubectl. 

\begin{figure}[H]
\centering
\begin{boxedverbatim}
FROM s390x/ubuntu:18.04 AS kub-build
 
# The author
MAINTAINER Sarah Julia Kriesch <sarah.kriesch@ibm.com>

#Installation
RUN echo "Installing necessary packages" && \
apt-get update && apt-get install -y \
apt-transport-https \
apt-utils \
systemd \
curl \
git \
ca-certificates \
gnupg-agent \
software-properties-common \
&& curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - \
&& echo "deb https://apt.kubernetes.io/ kubernetes-xenial main" \
> /etc/apt/sources.list.d/kubernetes.list \
&& apt-get update && apt-get install -y \
docker.io \
kubelet \
kubeadm \
&& apt-mark hold kubelet kubeadm kubectl \
&& apt-get clean \
&& rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/* \
&& systemctl enable docker 
\end{boxedverbatim}
 \caption{Kubernetes Installation}
    \label{kubernetes-installation}
\end{figure}

\section{Installation of the Latest Go}

There were some issues with older Go versions as 1.10 during building tests for Kubernetes. Therefore a higher version (min. 1.13) should be used. It is recommended to use the latest Go version for last versioned Kubernetes tests. It is possible to receive the version number of the last Go release with the command \\ 
\lstinline!curl https://golang.org/VERSION?m=text!. \\ 
This version number has to be included before linux-s390x.tar.gz for downloading the special s390x archive from the Go directory by \url{dl.google.com}. Then the version number has to be called with curl inside of another curl command with the whole path to the special tar archive on \url{dl.google.com}. Every tar archive has got the same structure for every version (\lstinline!$version.$architecture.tar.gz!). On this way the latest version of Go is integrable into the curl command that it can be installed. Directories for bin, pkg and src have to be created after extracting this tar archive in the \path{/root/} directory. They are not integrated in the tar archive. \\

The environment variables for GOROOT, GOPATH and PATH have to be set with ENV on the top of the Dockerfile for successful builds later. PWD is added because Github repositories have to be cloned to this directory. \\

The ENV variables will be on the top of the Dockerfile. The part for the "Installation of Go" will be attached to the end of the Kubernetes installation part.

\begin{figure}[H]
\centering
\begin{boxedverbatim}
ENV GOROOT=/root/go
ENV GOPATH=/root/go
ENV PATH=$GOPATH/bin:$PATH
ENV PATH=$PATH:$GOROOT/bin
ENV PWD=/root/go/src/

#Installation of latest GO
&& echo "Installation of latest GO" && \
curl "https://dl.google.com/go/ \
$(curl https://golang.org/VERSION?m=text).linux-s390x.tar.gz" \
| tar -C /root/ -xz \
&& mkdir -p /root/go/{bin,pkg,src} 
\end{boxedverbatim}
 \caption{Go Installation}
    \label{go-installation}
\end{figure}

\section{Building Tests}
After a successful installation of go, it is possible to build and install the Kubernetes test environment.
At first the directory k8s.io has to be created because kubernetes-tests are looking for this directory as a mother directory. The repository test-infra by the Kubernetes project has to be cloned to there. 
The repository \textbf{test-infra} contains all tests for Kubernetes provided by the community. 
These can be used of course and are updated continuously. That is the reason to clone this repository inside of the Dockerfile. Inside of this test-infra directory kubetest can be installed with \textbf{go install}. 
That is downloading all available Kubernetes-Tests. So you can use them to test the own Kubernetes cluster and the used software. \\

The name of the most relevant tests for the Kubernetes community is "conformance tests". These conformance tests are executed with e2e.test  which can be built with make inside of the kubernetes repository. 
Therefore this repository has to be cloned to k8s.io, too. These tests certify the software to comply regular standards. Only with complying these standards, Kubernetes software is allowed to become Kubernetes certified\footnote{\url{https://github.com/cncf/k8s-conformance}}. 


\begin{figure}[H]
\centering
\begin{boxedverbatim}
&& cd $PWD \
#Clone test-infra
&& mkdir -p $GOPATH/src/k8s.io \
&& cd $GOPATH/src/k8s.io \
&& git clone https://github.com/kubernetes/test-infra.git \
/root/go/src/k8s.io/test-infra \
&& cd /root/go/src/k8s.io/test-infra/ \
#Install kubetest
&& GO111MODULE=on go install ./kubetest \
#Build test binary
&& git clone https://github.com/kubernetes/kubernetes.git  \
/root/go/src/k8s.io/kubernetes \
&& cd /root/go/src/k8s.io/kubernetes/
 
CMD make WHAT="test/e2e/e2e.test vendor/github.com/onsi/ginkgo/ginkgo cmd/kubectl"
\end{boxedverbatim}
 \caption{Test Building for Kubernestes}
    \label{test-building}
\end{figure}

\subsection{End To End Testing}
The e2e.test suite is a end-to-end testing framework for Kubernetes. It is testing Kubernetes for all required functionality \cite{Ohly2019}. This framework is written in Go and is using the Ginkgo Testing framework\footnote{\url{https://onsi.github.io/ginkgo/}} for expressive and comprehensive tests with the style of Behavior Driven Development (BDD).
Expected behaviors are described in specs inside of the Kubernetes directory for e2e-tests\footnote{\url{https://github.com/kubernetes/kubernetes/tree/master/test/e2e}}. These tests exist for every Kubernetes version in specified Github branches. \\
The test is built with the installed Go based on the file \textbf{e2e\_test.go}. Kubernetes has been importing all multiple providers, in-tree tests, configuration support, and bindata file lookup in the file e2e\_test.go.
The vendoring code and their dependencies are available under \path{k8s.io/kubernetes/vendor/}. Additionally, these can be tested limited to necessary dependencies. \\
Conformance tests can be included with: \\
\begin{figure}[H]
\centering
\begin{boxedverbatim}
export KUBERNETES_CONFORMANCE_TEST=y
export KUBECONFIG=”$HOME/.kube/config”
go run hack/e2e.go -- --provider=skeleton --test \
--test_args=”--ginkgo.focus=\[Conformance\]” 
\end{boxedverbatim}
 \caption{E2e-Test}
    \label{e2e-test}
\end{figure}

These conformance tests are a subset of e2e-tests \cite[~p.8]{Omichi2018}. Every vendor is receiving a certification by Kubernetes with passing all required conformance tests. 167 of 999 tests had been such conformance tests in the year 2018 \cite[~p.9]{Omichi2018}.
A working Kubernetes test setup is required for running tests against it.

\section{Run in the Main Dockerfile}

\section{Integration Tests}

\section{Integration into CI/CD}

The Kubernetes project has been using Prow as a CI/CD system. Prow is a Kubernetes based CI/CD system further developed based on Jenkins \cite{JAXenter}. That implifies, that the existing CI/CD pipeline can work only with containers.
Before the integration into the community pipeline, it is possible to use a default Jenkins system because of identical configuration files. An additional advantage in this case is the chance to test the integration of Kubernetes into QEMU before providing the IBM Z environment container based for test purposes.

\subsection{Jenkins versus Prow}

As mentioned in \ref{CI-CD} "CI/CD Systems", \textbf{Jenkins}\footnote{\url{https://www.jenkins.io/}} is the most popular open-source CI/CD system. This open-source project has started with a default CI/CD workflow based on real hardware and later on virtual machines to test software before the release. Jenkins provides many plugins\footnote{\url{https://plugins.jenkins.io/}} for git integration about release management support until container deployments. \\
There exists a special \textbf{Kubernetes plugin}\footnote{\url{https://plugins.jenkins.io/kubernetes/}}, that Jenkins can understand yaml configuration files for Kubernetes deployments which are used for integration tests by the Kubernetes community as an example. On this way, Jenkins can execute the same tests as Prow together with emulations because the Kubernetes environment is deployed inside of a emulated system. Jenkins can handle emulated systems the same as virtual machines. The Kubernetes plugin makes automated Kubernetes deployments possible. Therefore, a Jenkins installation is the best choice for the transition from emulated systems together with container technologies to a working test strategy with container based test environments in Prow.

\textbf{Jenkins X}\footnote{\url{https://jenkins-x.io/}} has been developed based on Jenkins specialized on Kubernetes and cloud native environments. That means, that something as the Kubernetes plugin is integrated and the system can work with public cloud providers and other cloud technologies for test and production deployment pipelines. All in all, Jenkins X is an improved version of Jenkins for cloud environments. The system can work with container registries, understands yaml configuration files without additional plugins and is able to deploy scalable systems based on containers in the local data centre as well as in the public cloud (AWS, Google Cloud, IBM cloud, Azure).

\textbf{Prow}\footnote{\url{https://jenkins-x.io/docs/reference/components/prow/}} is a Kubernetes-native solution based on Jenkins X. This CI/CD system is used by projects as Kubernetes, Istio und OpenShift which are all based on Kubernetes. This system is something as a Jenkins X expanded with special Kubernetes APIs and cloud specific features. It can understand and deploy only Kubernetes based environments. Other configurations are not supported. The benefit in comparison to Jenkins is, that yaml configurations for Kubernetes setups are transferable between Jenkins and Prow.

\subsection{System Requirements}

Jenkins can include all required installation of integration tests and the whole system setup into the specified Docker image. Therefore, the disk image size can be defined based on the Docker image and the specified command in \ref{ExternalFile system} "Mounting an External File system" with \\
\lstinline!docker images | grep 'Kubernetes-Test' | awk '{print int($7+0.5)"G"}'! \\

The Kubernetes community is referencing\footnote{\url{https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/\#before-you-begin}} a minimum about 2GB of memory and 2 CPUs per system. 
It has to be observerd, that Kubernetes is watching master and worker node as a own system. Both are on one system in our case. Additionally, some RAM and CPU should be added for running tests. Therefore, 6GB for memory and 5 CPUs will be calculated for the QEMU setup.